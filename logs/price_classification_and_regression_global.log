/Users/Shared/PyCharmProjects/venv/deeplearning/bin/python /Users/Shared/PyCharmProjects/kc-tfb/price_classification_and_regression_global.py --extract_features True
Tamaño reviews (8768, 2)
sys:1: DtypeWarning: Columns (43,61,62) have mixed types.Specify dtype option on import or set low_memory=False.
Tamaño listings (21739, 106)
Tamaño listings después de join (8768, 108)
Tamaño df_images (11304, 2)
Tamaño listings después de join final (8768, 110)
Verificamos si hay nulos en el dataset resultante
listing_id                      0
price                           0
vader_sentiment                 0
host_response_rate           2009
latitude                        0
longitude                       0
bathrooms                       5
bedrooms                        1
beds                            2
guests_included                 0
extra_people                    0
minimum_nights                  0
maximum_nights                  0
availability_365                0
number_of_reviews               0
host_total_listings_count       0
property_type                   0
room_type                       0
bed_type                        0
neighbourhood_cleansed          0
cancellation_policy             0
filename                        0
dtype: int64


Vemos que tenemos valores nulos en host_Response_rate, bathrooms, bedrooms, y beds. Vamos a asignales el valor 0, ya que consideramos que si no se han indicado valor es porque su valor real es 0.
Verificamos de nuevo que ya no hay nulos

listing_id                   0
price                        0
vader_sentiment              0
host_response_rate           0
latitude                     0
longitude                    0
bathrooms                    0
bedrooms                     0
beds                         0
guests_included              0
extra_people                 0
minimum_nights               0
maximum_nights               0
availability_365             0
number_of_reviews            0
host_total_listings_count    0
property_type                0
room_type                    0
bed_type                     0
neighbourhood_cleansed       0
cancellation_policy          0
filename                     0
dtype: int64
Examinamos los valores numéricos del dataset

                            count          mean           std         min  \
listing_id                 8768.0  2.052828e+07  1.128771e+07  6369.00000
price                      8768.0  1.192326e+02  4.500888e+02     9.00000
vader_sentiment            8768.0  9.140426e-01  1.339225e-01     0.00000
latitude                   8768.0  4.042012e+01  2.034580e-02    40.33247
longitude                  8768.0 -3.696168e+00  2.480543e-02    -3.86391
bathrooms                  8768.0  1.285983e+00  5.962362e-01     0.00000
bedrooms                   8768.0  1.371464e+00  8.849102e-01     0.00000
beds                       8768.0  2.070826e+00  1.669342e+00     0.00000
guests_included            8768.0  1.914234e+00  1.509164e+00     1.00000
minimum_nights             8768.0  5.084740e+00  2.911264e+01     1.00000
maximum_nights             8768.0  1.481401e+04  1.192552e+06     1.00000
availability_365           8768.0  1.714824e+02  1.397992e+02     0.00000
number_of_reviews          8768.0  6.062055e+01  8.057472e+01     1.00000
host_total_listings_count  8768.0  1.744742e+01  6.528542e+01     0.00000

                                    25%           50%           75%  \
listing_id                 1.175356e+07  1.930310e+07  3.046512e+07
price                      3.800000e+01  6.000000e+01  9.000000e+01
vader_sentiment            9.000000e-01  9.615385e-01  1.000000e+00
latitude                   4.040986e+01  4.041805e+01  4.042707e+01
longitude                 -3.707372e+00 -3.701490e+00 -3.692988e+00
bathrooms                  1.000000e+00  1.000000e+00  1.500000e+00
bedrooms                   1.000000e+00  1.000000e+00  2.000000e+00
beds                       1.000000e+00  2.000000e+00  3.000000e+00
guests_included            1.000000e+00  1.000000e+00  2.000000e+00
minimum_nights             1.000000e+00  2.000000e+00  3.000000e+00
maximum_nights             6.000000e+01  1.125000e+03  1.125000e+03
availability_365           1.000000e+01  1.700000e+02  3.280000e+02
number_of_reviews          8.000000e+00  2.800000e+01  7.900000e+01
host_total_listings_count  1.000000e+00  2.000000e+00  7.000000e+00

                                    max
listing_id                 4.012376e+07
price                      9.999000e+03
vader_sentiment            1.000000e+00
latitude                   4.056274e+01
longitude                 -3.528920e+00
bathrooms                  9.000000e+00
bedrooms                   1.200000e+01
beds                       5.000000e+01
guests_included            1.600000e+01
minimum_nights             1.125000e+03
maximum_nights             1.111111e+08
availability_365           3.650000e+02
number_of_reviews          6.610000e+02
host_total_listings_count  5.030000e+02

Podemos observar que hay unos máximos excesivos en price y maximum_nights, que podemos considerar outliers


Vamos a eliminar las filas dichas filas

Número de listings con precio superior a 3000 59
Número de listings con máximo número de noches superior a 3000 7

Consideramos precios superios a 3000 erróneos y también maximo número de noches a 2000. Borramos dichas filas

Tamaño final del dataset de listings (8702, 22)
Cargamos las imágenes mosaico de cada listing en memoria
100%|██████████| 8702/8702 [00:06<00:00, 1368.89it/s]
Imágenes cargadas en memoria. Tiempo transcurrido total en segundos  6.5531697273254395
2020-08-06 22:49:55.816057: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-06 22:49:55.833368: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd78962dd90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-06 22:49:55.833385: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Procedemos a extraer las características de la imágenes vinculadas a análisis de sentimiento
100%|██████████| 8702/8702 [28:08<00:00,  5.15it/s]

Tamaño final del dataset de listings (8702, 22)
Tamaño del array de característcias de imágenes (8702, 512)
Convertimos el array numpy de característcias de imágenes a un dataframe  y verificamente el tamaño (8702, 512)
Primeras filas del dataframe de características de imágenes
          0         1         2         3         4         5         6  \
0  1.787984  0.000000  0.550765  5.020306  0.019869  0.000000  1.586092
1  1.956667  0.000000  0.854298  4.454948  1.504656  0.306371  4.197949
2  0.000000  0.000000  0.000000  0.000000  0.899871  0.217655  2.572991
3  0.188184  0.000000  0.000000  0.000180  0.468999  0.001272  1.840106
4  0.470532  0.344944  4.704133  0.877038  0.436312  0.000000  2.501666

          7         8         9        10        11         12   13        14  \
0  1.968155  0.080452  6.941063  0.076232  0.896289  12.892452  0.0  0.100943
1  0.282891  0.000000  3.632165  0.000000  1.304026   7.403183  0.0  0.032194
2  0.519623  0.000000  3.492013  0.000000  0.080415   5.523358  0.0  0.922783
3  0.711625  0.000000  3.012746  0.000000  0.000000  12.129988  0.0  0.498740
4  0.476869  0.000000  4.155890  0.836593  0.847802   2.355654  0.0  0.566609

         15        16        17         18        19         20        21  \
0  8.725959  1.762905  1.612285   0.167442  2.241052  12.670854  0.000000
1  4.060488  0.000000  5.591368   3.558830  6.628867   5.239213  2.870082
2  1.850194  0.000000  0.356039   0.000000  0.386819   1.673026  1.342882
3  0.642114  0.000000  0.534453   0.085374  1.546641   0.713491  1.524802
4  2.016182  2.083679  0.818207  16.706100  0.460598   2.303580  3.334275

          22        23        24        25        26        27        28  \
0  11.593314  2.787676  0.000000  3.586055  4.352153  0.008267  2.857925
1   6.464534  0.356101  0.977804  0.462541  7.993815  3.433822  3.426430
2   0.499085  0.000000  0.000000  0.000000  5.497185  1.734796  2.477766
3   0.043627  1.103532  0.000000  0.000000  8.744786  1.772693  3.036064
4   3.758706  0.483902  1.322817  1.608664  1.367806  0.364904  0.831145

         29         30        31        32        33        34        35  \
0  0.166914   5.254900  1.886797  2.715858  4.393841  9.933576  2.309909
1  0.601865  17.814131  3.203612  2.614537  2.310109  4.031092  6.667244
2  1.127611  17.365477  0.801418  5.443479  0.000000  4.774827  0.534238
3  0.024286   8.617540  0.000000  1.263405  0.195541  7.865283  4.787516
4  0.000000  16.378492  0.000000  0.066265  0.630220  2.767879  1.179641

         36        37        38        39        40        41        42  \
0  0.000000  0.710073  0.137324  0.000000  1.868355  0.000000  3.228909
1  0.050039  1.049884  0.000000  0.184413  1.086372  0.077598  9.736145
2  0.176376  0.485429  0.000000  0.000000  0.368437  0.000000  4.156777
3  0.000000  1.635146  0.000000  0.280412  0.000000  0.423026  1.849407
4  1.944717  7.333575  5.620854  0.444279  0.081644  0.000000  0.373859

         43        44        45        46        47        48        49  \
0  0.349770  6.543718  0.426273  0.445654  0.992508  0.869624  1.248829
1  3.150966  1.592670  0.817213  0.612798  0.189472  0.311727  1.847872
2  0.000000  0.191319  0.647837  0.931836  1.048087  4.396898  0.000000
3  0.274124  0.360663  0.002255  1.003882  0.833520  0.000000  0.578331
4  0.538546  1.644058  0.000000  2.246246  0.373235  4.405001  0.323850

         50         51         52        53        54        55        56  \
0  3.200384   2.059827   2.728805  3.087382  2.592159  2.646377  0.000000
1  1.701677  11.454160   9.192422  1.021615  3.453180  1.959627  0.000000
2  6.607305  16.532963   7.955967  0.111756  0.000000  1.183777  0.076658
3  6.914241  16.255106   1.924740  1.785672  5.343620  1.940120  0.000000
4  3.496234  12.470716  19.739790  3.088969  0.549880  2.507269  0.455096

         57         58         59        60        61        62        63  \
0  7.590399   0.346209   4.347166  0.028721  0.073821  0.069711  9.627875
1  0.229848   0.606706  10.018786  0.000000  4.914753  0.000000  5.489703
2  1.996357   1.179635   0.069043  0.000000  0.697761  1.944677  3.256931
3  9.584954   2.029360   4.613770  0.000000  1.834153  1.274325  2.765591
4  7.622589  10.683184   0.000000  0.000000  7.637832  3.275975  6.459870

         64        65         66        67         68        69        70  \
0  0.469803  0.441336   0.000000  0.399287   0.051702  0.220965  0.000000
1  0.387298  0.000000   0.302139  1.172642   2.057711  0.268725  0.000000
2  1.500309  0.000000   0.000000  5.179030   0.347345  0.808332  0.000000
3  4.090232  0.000000   0.000000  2.344167   1.407064  0.764493  0.000000
4  2.244386  0.483276  11.043159  0.760422  11.987367  0.409739  0.070472

         71        72         73        74        75        76         77  \
0  3.307853  0.957433   9.527573  0.204744  2.579813  3.993388   5.617360
1  9.206648  0.251240   0.397780  4.323674  5.808261  0.057414   9.395776
2  2.850987  0.803930   1.433476  9.816840  6.822960  0.123526  16.691685
3  2.894328  0.226651   5.115034  0.162144  0.220091  0.079700  14.837865
4  4.885725  6.818340  12.879354  0.142554  2.092105  0.000000   3.034116

          78        79   80        81        82        83        84  \
0   5.127483  0.424067  0.0  1.565032  0.000000  4.729028  0.073352
1   3.558436  0.597822  0.0  0.000000  0.510582  0.309448  1.494759
2   8.828860  0.278778  0.0  2.154568  5.506942  0.000000  0.000000
3  26.535301  1.013853  0.0  0.000000  0.380639  0.140120  0.000000
4   1.787630  0.588169  0.0  2.814233  3.624482  7.337852  0.000000

          85        86        87        88         89   90        91  \
0  12.267648  0.536976  0.000000  5.203795   9.259501  0.0  1.657772
1   6.750800  4.752422  0.005827  0.528643  10.421758  0.0  0.000000
2   0.556045  3.790658  2.130723  0.000000  10.504683  0.0  0.000000
3   6.442552  0.000000  0.000000  0.161110   6.201326  0.0  0.000000
4  12.482547  3.722800  0.000000  2.098814   2.062548  0.0  0.078497

          92        93         94        95        96        97        98  \
0   0.960766  0.000000   0.221405  3.740505  0.718714  0.000000  1.106352
1   4.569354  0.000000   2.925335  5.083776  1.188792  0.948667  2.120024
2  15.518429  0.000000  10.249496  2.719997  2.778817  0.000000  0.000000
3   4.762357  0.228971   0.542707  2.920186  1.856228  0.000000  7.421423
4   3.981738  0.862878  10.110337  2.660573  2.795784  3.375427  2.995450

         99       100        101       102       103       104       105  \
0  7.866215  0.229782  17.220961  0.000000  2.610302  0.862181  4.878597
1  4.621658  0.233901  29.386816  0.000000  0.006918  0.000000  0.000000
2  1.327223  0.000000  18.351469  0.000000  0.000000  0.101630  2.104839
3  0.807189  0.000000  15.056805  0.000000  0.024094  0.931568  0.000000
4  6.131334  0.153251   9.649803  0.164484  1.391926  3.741310  2.482108

        106        107       108       109       110  111       112  \
0  1.262516   0.284236  1.335684  2.061455  2.271698  0.0  4.356859
1  0.384326   4.738772  2.969067  0.294534  5.374464  0.0  0.000000
2  0.034401   1.184735  2.993469  0.000000  0.148229  0.0  1.061004
3  0.127469   0.538679  2.893124  0.597773  4.527530  0.0  0.558729
4  6.504690  14.107234  1.700550  0.336895  1.180143  0.0  1.631747

         113       114       115       116       117       118       119  \
0   6.921042  3.246405  0.425661  0.000000  2.254542  2.044357  1.498566
1  13.487711  1.578381  1.380500  0.090696  1.291929  0.026925  2.015125
2   3.114064  2.512921  4.714334  0.852307  0.421414  1.143604  0.249352
3   2.886340  3.372448  3.945254  0.000000  0.832263  2.050668  1.234504
4   3.662547  3.611127  6.363365  0.000000  2.016388  2.455703  8.193419

         120       121       122       123        124        125       126  \
0   3.391213  0.000000  0.000000  1.292741   2.651232   1.695220  0.000000
1   7.413363  0.558294  0.000000  2.815394   4.491860   0.404118  0.000000
2   8.255198  2.302235  0.000000  0.651012   6.588274   6.779811  0.000000
3   7.766368  1.967425  0.000000  0.468703   2.201495   2.275073  0.000000
4  16.229053  5.354609  1.115734  1.326922  10.966600  12.203650  0.869547

        127       128        129       130       131        132       133  \
0  6.921424  1.701750  17.924982  0.000000  0.605593   5.082107  0.378794
1  0.000000  3.721349   6.430328  0.115355  2.189781   3.028963  0.578071
2  0.014280  0.918111   3.446501  2.044180  0.000000  17.477358  0.000000
3  0.019341  1.274709   7.138769  1.040062  0.000000   6.037504  0.000000
4  2.498360  3.260243   5.428142  2.609804  0.000000   0.287386  0.242400

        134       135       136       137       138       139       140  \
0  1.385626  0.512792  1.412725  1.153663  0.306176  0.305927  1.271698
1  0.034147  0.000000  3.008410  2.290695  3.770869  1.475911  3.756733
2  0.000000  0.000000  2.939555  0.963470  1.201465  0.089804  0.327921
3  0.000000  0.000000  1.057546  0.985601  1.582159  0.125543  3.013089
4  0.729964  0.206449  0.177454  0.529264  6.055752  3.905047  5.355330

        141       142       143       144  145       146       147       148  \
0  0.407676  0.000000  0.165068  1.093400  0.0  5.281242  0.000000  5.185062
1  0.000000  0.124232  0.190505  1.788343  0.0  3.100096  0.310243  7.464800
2  0.000000  2.445880  0.000000  8.510270  0.0  1.148263  0.774993  2.142553
3  0.000000  4.595836  0.000000  0.000000  0.0  1.955924  1.190299  3.205015
4  0.278955  2.417727  0.000000  0.974347  0.0  2.722371  2.173729  1.471321

        149       150       151       152       153       154        155  \
0  0.640338  1.942236  0.317589  2.378462  1.843451  0.949781  14.752972
1  0.000000  0.756418  0.000000  8.920465  0.324966  0.903518  11.281039
2  0.230407  0.381654  0.000000  0.843420  0.245121  1.938455   9.754374
3  0.398934  0.055438  0.000000  2.667004  0.000000  0.000000  12.273608
4  1.029526  0.347190  0.000000  4.601135  0.000000  0.269031  16.419085

        156       157  158       159       160       161        162  \
0  0.000000  0.101114  0.0  0.465615  0.000000  1.571500   0.400452
1  0.169942  0.000000  0.0  1.425643  0.008944  1.337774  13.153791
2  2.391454  0.000000  0.0  0.716103  0.000000  0.000000   8.049970
3  1.786145  0.000000  0.0  0.000000  0.046666  0.232817   9.009092
4  3.423817  0.145734  0.0  0.000000  0.361191  4.146146   0.246822

         163       164       165  166  167       168       169        170  \
0   7.469190  1.902581  4.913409  0.0  0.0  3.019216  0.000000   0.642518
1  14.908656  1.411226  2.414134  0.0  0.0  3.192114  0.054547   0.000000
2   3.547992  1.243947  1.003345  0.0  0.0  1.985441  0.000000   0.380623
3  10.994728  0.112949  1.700680  0.0  0.0  2.360295  0.000000   0.479025
4   4.055895  0.000000  5.383527  0.0  0.0  1.845509  0.088748  12.797062

        171       172        173       174        175        176       177  \
0  0.072960  0.924147  24.978558  2.886749  16.022455   3.415682  3.001478
1  0.000000  4.873905  15.605446  0.295112   6.560826   3.324000  3.753346
2  0.000000  3.798964  12.867826  0.000000   2.422202   1.488839  0.187371
3  0.000000  3.831568  31.387737  0.000000   7.086331   0.143181  2.015960
4  0.945521  1.387901  16.613461  3.769865   8.023605  17.917944  8.923943

         178       179       180        181       182       183        184  \
0   0.506736  0.000000  2.917917   0.000000  0.242945  0.333032   2.696275
1   3.001238  0.000000  0.375633   5.339992  0.437362  0.289552   4.875627
2   0.655629  0.592428  0.000000  10.908908  0.527743  0.001792  10.906552
3   0.006455  0.321755  1.049284   0.805377  0.217630  0.000000   7.076931
4  11.472633  0.298915  0.326497   0.272780  0.581458  0.330045   0.431659

        185        186       187       188       189       190       191  \
0  0.843511   8.191612  1.369467  0.068861  0.000000  1.063043  1.239744
1  0.477956  11.195315  1.174241  0.261767  0.000000  2.034281  2.557638
2  0.000000   1.014064  1.237968  0.122147  0.000000  2.260407  0.740247
3  0.000000   1.068767  5.971649  0.629994  0.145585  0.000000  0.000000
4  1.511022   0.246682  1.640998  0.079657  0.000000  0.000000  2.458925

        192       193       194       195        196       197  198       199  \
0  5.030432  0.622556  0.500379  0.097392  11.886683  0.000000  0.0  0.000000
1  4.982244  1.127170  3.086820  0.075800   3.536383  0.056502  0.0  0.000000
2  1.355339  0.000000  3.837941  0.000000   2.111171  0.139235  0.0  0.000000
3  1.530163  1.017882  1.727051  0.000000   6.347518  0.000000  0.0  0.000000
4  9.745467  0.140175  9.060932  0.000000   0.349361  3.744364  0.0  0.508682

        200        201       202       203       204        205       206  \
0  0.314952   4.197372  0.073409  0.360591  3.599267   0.559656  1.880259
1  0.000000   7.128461  1.076953  3.082781  2.748396   0.214571  0.607932
2  0.000000  10.072366  0.000000  3.446289  0.296091   1.338181  1.222270
3  0.000000   2.187633  0.153321  1.260594  0.456938   0.000000  0.510710
4  0.000000  12.604587  3.016516  3.844532  2.470553  12.926035  0.748035

        207       208       209       210       211       212       213  \
0  1.630283  0.856767  0.118367  2.363273  6.160290  6.119093  0.067935
1  0.645241  0.000000  4.316672  3.497432  5.793594  0.214655  1.944104
2  2.262899  0.123031  1.500687  9.072643  5.528120  0.266593  1.915195
3  2.360526  0.000000  3.814236  1.052979  1.439783  2.264113  5.365396
4  3.607412  0.127983  0.399093  0.301496  9.347420  2.369740  4.910864

        214        215       216       217       218       219       220  \
0  0.066763   8.350403  0.163999  7.132518  2.017674  2.656302  0.391009
1  0.000000  16.868950  1.757366  1.985595  2.207144  4.920543  0.282580
2  0.000000  18.806257  6.032284  1.740170  3.873359  1.622770  0.042397
3  0.000000  14.741668  0.453401  3.217561  1.295474  2.546808  0.000000
4  0.000000  16.745420  3.279250  1.029858  6.288651  0.729361  0.069198

        221       222       223       224       225       226        227  \
0  0.509307  0.000000  0.164382  0.123664  8.027192  0.127945  10.077700
1  1.795216  1.317889  0.032645  3.351669  0.074586  0.851765   8.303308
2  0.279713  0.051939  0.052480  0.000000  3.169636  1.517292   4.116816
3  0.000000  0.000000  1.242383  0.000000  1.291737  0.302648   7.272194
4  3.084180  0.119938  5.742597  0.000000  0.596380  4.465589   9.162923

        228       229       230       231       232       233       234  \
0  0.918194  2.383282  1.185619  2.960985  3.894423  0.000000  0.000000
1  0.255044  2.588067  1.311091  3.647886  6.869752  1.467476  0.000000
2  4.941022  2.811681  2.152007  0.060057  0.508771  0.011833  0.000000
3  3.578500  5.293357  8.014216  0.527601  0.642140  0.058100  0.000000
4  4.752787  1.233111  6.814533  7.586645  0.017472  0.000000  1.911031

        235       236       237       238       239       240       241  \
0  0.000000  0.000000  1.812512  0.000000  0.000000  1.123958  0.025648
1  0.909785  0.260016  1.203170  0.000000  0.104049  0.308363  0.750257
2  0.000000  0.000000  1.067567  0.061761  0.354025  0.874574  0.000000
3  0.000000  0.010108  2.073501  0.000000  0.000000  1.140375  3.629178
4  0.000000  0.256560  0.196462  0.325593  0.000000  2.656052  2.026910

        242       243       244       245       246       247       248  \
0  5.188384  1.513157  6.785950  0.000000  3.108866  0.044004  0.357618
1  2.641192  0.000000  1.047896  3.540009  1.141533  8.510078  0.609414
2  0.603281  0.000000  1.568135  3.001172  0.163800  0.277064  0.127195
3  6.706582  0.000000  0.155480  0.000000  0.568083  1.473339  0.000000
4  2.019328  2.099561  0.992152  2.215381  4.291236  0.633610  9.540422

        249        250        251        252        253       254       255  \
0  0.000000   6.962708   8.984765   0.174905   3.825471  1.577508  1.526273
1  8.271274  10.970572   0.070428   1.388949   3.849848  0.000000  5.327063
2  6.053478   6.078832   0.815898   2.118119   4.482481  0.323062  2.060444
3  2.768105   8.099729   0.000000   0.214755   1.056357  0.000000  0.328424
4  0.000000  28.406244  15.548633  29.940170  16.718853  0.047885  0.049414

        256       257        258       259       260       261       262  \
0  1.114805  4.164111   8.135581  1.362091  5.485558  8.097595  4.025180
1  1.482237  0.730581   8.043203  1.688706  1.434064  1.534846  4.470335
2  2.859221  2.013309   0.907103  0.800724  0.386582  0.925964  1.868183
3  1.606311  2.698476  10.125728  2.909721  3.074439  7.870416  4.786491
4  4.598106  6.860209  11.562885  3.729787  2.307530  0.837915  0.244246

        263        264       265        266       267      268       269  \
0  0.000000  24.314226  0.070377  14.315649  2.009711  0.13127  2.218129
1  0.000000  17.031269  0.000000   3.335869  3.634925  0.00000  0.631174
2  1.541391  10.486996  0.000000   3.844647  3.521963  0.00000  0.638781
3  3.003346  18.039751  0.000000   3.865528  5.126392  0.00000  0.144608
4  2.586568  18.638828  0.000000   6.935369  9.663120  0.00000  0.922973

       270       271       272       273       274       275       276  \
0  0.00000  2.561696  6.885581  2.611660  0.000000  0.392663  1.647872
1  0.00000  3.523402  4.869500  2.044938  0.000000  0.000000  3.872703
2  0.00000  1.046272  5.738727  0.519990  0.000000  0.000000  6.538076
3  0.00000  0.548377  1.494550  0.837322  0.000000  0.236621  3.486047
4  0.59266  0.811889  1.264767  7.557831  0.311742  0.012139  4.121205

        277       278        279  280        281       282        283  \
0  3.776168  3.587921   4.871976  0.0   5.226203  2.945668  20.731743
1  2.313642  0.476571   2.462796  0.0   1.796431  1.493984   1.127424
2  0.103930  0.000000   0.268651  0.0   0.153230  2.541609   3.655893
3  0.249001  0.000000   0.958063  0.0   0.685644  7.209859   3.356408
4  5.173325  0.867723  22.087145  0.0  22.661167  1.203483  14.049336

       284       285  286       287       288        289       290       291  \
0  0.00000  0.000000  0.0  0.000000  1.027390   3.303388  5.529913  1.959183
1  0.00000  0.871702  0.0  0.000000  0.289006   2.607106  0.022020  3.071141
2  0.00000  0.195843  0.0  0.375471  0.000000   0.003193  0.803658  0.375543
3  0.00000  0.000000  0.0  0.000000  0.000000   0.084104  5.377049  0.412173
4  4.67376  3.637632  0.0  0.159591  1.153337  17.360083  3.004767  0.000000

         292       293        294       295       296  297       298  \
0  26.804974  0.035766   5.651575  0.898489  0.670397  0.0  1.002284
1   8.901572  3.629678   4.047225  0.107494  1.872304  0.0  0.084381
2   5.571773  1.248246  20.435503  0.000000  1.016829  0.0  1.031591
3   2.764264  0.704765   4.420989  0.000000  2.288669  0.0  0.471131
4  15.102113  0.010418   5.191744  0.407846  0.453690  0.0  0.975971

        299       300       301        302       303       304       305  \
0  0.398282  8.644524  0.000000   1.933222  1.485716  0.175488  0.537889
1  0.000000  2.149567  0.817784   8.755590  0.180190  1.259128  0.678135
2  3.881952  3.232434  0.064636   6.104635  0.659143  4.298564  0.845250
3  0.223049  7.551022  0.000000   2.486838  0.790426  0.470345  0.136397
4  0.000000  1.991053  2.486553  14.396099  0.000000  3.937727  1.869587

         306       307  308        309       310       311        312  \
0   1.353411  1.989330  0.0   5.653601  0.000000  0.000000  11.057096
1   2.264784  2.262332  0.0   0.974765  0.426512  0.000000  14.556633
2  11.654679  2.019186  0.0   2.376862  0.332139  0.202117  11.774605
3   0.802160  4.041625  0.0   4.533832  0.362325  0.215142   5.043033
4   2.651915  2.329539  0.0  26.260403  2.966507  7.160054   2.717610

        313        314       315       316       317       318       319  \
0  0.234621   0.198573  0.227741  0.445853  1.442328  0.008062  5.721504
1  0.000000   1.243373  0.374842  0.282825  0.105948  0.000000  7.362982
2  0.000000   2.001401  1.124816  0.874633  0.000000  0.000000  2.538479
3  0.000000   1.787896  0.487860  0.000000  1.428958  0.000000  0.000000
4  0.000000  13.303939  1.513293  1.649233  2.050695  0.000000  0.723682

        320       321       322       323        324  325        326  \
0  0.390284  1.203765  0.000000  0.000000   8.635112  0.0   5.539785
1  0.751218  0.346843  2.937733  1.712017   2.239265  0.0   0.586162
2  0.000000  2.628884  0.482233  0.000000  15.068909  0.0   2.316188
3  0.000000  1.338823  0.015957  0.000000   1.436746  0.0  10.323509
4  0.600858  2.432935  0.661767  1.109866   1.371844  0.0  10.354958

        327       328       329       330       331       332       333  \
0  5.393512  2.022573  0.000000  0.821408  0.804353  2.451469  0.335882
1  1.700525  1.512050  0.000000  0.000000  1.925929  0.881220  0.266759
2  0.000000  2.806692  0.000000  0.000000  0.851479  0.364755  0.000000
3  1.085508  0.692596  0.000000  0.000000  0.755653  0.089953  0.070174
4  7.774240  1.546781  1.273888  0.830699  6.907452  3.348992  1.098634

         334       335       336       337       338       339       340  \
0  23.681814  5.202618  3.168665  3.763858  5.431021  0.021338  7.652653
1  30.773411  7.208980  0.037834  1.688725  0.212026  0.175030  7.222723
2  42.958576  0.990543  0.000000  2.613268  0.000000  0.000000  1.514910
3  29.637232  0.584954  0.000000  2.503391  3.219486  0.000000  1.353175
4  32.663807  2.603647  1.957970  0.385274  3.069709  0.129614  4.282137

        341       342        343        344       345       346       347  \
0  1.056100  1.241378   4.783664  12.694362  0.525010  4.082468  7.184873
1  0.080345  4.299454   6.128170   3.331144  0.000000  0.192610  4.699991
2  1.790551  9.345789  15.476998   0.300280  0.583315  0.000000  1.477828
3  0.583149  5.464301   2.571589   3.211071  0.006343  0.226835  0.307963
4  1.732770  7.134089   0.065177  27.139103  2.237126  3.450666  4.251944

        348       349       350       351        352        353       354  \
0  0.187675  0.000000  2.328841  1.588387   4.022717   6.428221  0.970044
1  0.037543  0.705334  2.777909  0.000000   8.654314  16.404657  0.015561
2  0.000000  0.076342  1.143142  3.038440   8.205536   8.833723  0.613908
3  0.000000  0.000000  1.817400  1.454725  15.397348  20.423416  0.000000
4  3.118872  1.670867  3.691383  0.786110   7.879731   5.135527  1.437880

        355       356       357        358       359        360       361  \
0  4.698148  0.182095  0.239749  10.096421  1.136667  14.530138  0.062559
1  0.367022  2.976093  0.187633  10.152925  2.854323   6.027895  0.000000
2  4.514696  0.093972  0.000000   0.472090  1.325794  10.805857  0.000000
3  3.806019  0.723313  0.000000  18.480057  3.252308  12.600978  0.000000
4  3.442874  3.933955  1.786870  14.428493  7.123028  13.555314  0.000000

        362       363  364        365       366       367       368  \
0  0.000000  1.819080  0.0  11.473869  1.700540  0.000000  0.315418
1  0.000000  2.086696  0.0   1.983203  1.072303  0.304684  0.869265
2  0.000000  0.781024  0.0   9.063019  8.044851  0.028120  0.000000
3  0.141018  3.964638  0.0   1.438849  1.017553  0.022470  0.000000
4  0.000000  0.855562  0.0   8.901388  2.065972  1.807330  2.629464

         369       370       371       372       373       374        375  \
0   1.698696  0.000000  0.658688  3.154470  4.526463  0.580571   0.256692
1   0.295290  0.000000  1.068026  0.000000  0.000000  2.450304   1.585990
2   6.331357  0.000000  0.557378  0.080511  0.132720  0.430961   2.641755
3  13.672938  0.000000  0.626273  0.349153  0.126432  0.054309   0.031979
4   9.736710  0.425767  1.281975  3.031610  1.273750  0.114305  10.173548

        376        377        378       379       380        381       382  \
0  0.000000   2.246180   4.981061  0.058386  0.393168   4.683932  1.403861
1  0.000000  15.175070   9.165052  0.314802  0.000000   4.450978  3.010856
2  0.000000   1.180046  20.905373  0.032011  2.043255   5.988930  0.303919
3  0.000000   0.162142   1.512455  0.000000  0.000000   6.558942  0.063242
4  0.083597   1.470660   7.773294  6.137022  0.835307  16.444582  0.000000

        383        384       385       386        387        388       389  \
0  0.000000  13.111609  0.524089  4.110533   2.473474   0.591919  0.000000
1  0.000000   8.754553  0.066860  4.153861   3.174657   3.416993  0.000000
2  0.000000  15.032551  0.000000  0.484860   2.724611   1.946809  0.365560
3  0.028899   7.923435  0.012795  1.173369   3.387621   8.906822  0.000000
4  0.000000  12.122383  4.528545  9.711509  29.255592  16.128593  0.334639

        390       391       392       393       394       395        396  \
0  8.009454  2.445857  0.574256  0.000000  0.318063  2.090968   3.429822
1  9.948538  5.215435  0.000000  0.117409  1.238026  0.797882   5.147259
2  1.469481  3.928177  0.000000  0.006997  2.004990  4.256122   2.259808
3  0.439403  0.108692  1.746664  1.294706  0.210194  0.000000  15.496947
4  0.656849  2.729134  2.205855  2.057321  3.108559  0.634070   2.467584

        397       398       399       400       401       402       403  \
0  0.440713  0.713790  0.331609  0.366844  3.958367  1.094669  0.000000
1  0.000000  0.310260  4.012693  1.838274  9.942306  0.009886  0.892436
2  0.000000  1.895060  0.000000  0.258920  1.942406  0.646242  0.216583
3  0.000000  0.197824  0.000000  0.462698  2.003385  0.551947  0.000000
4  3.586136  3.035497  0.000000  6.168378  2.533563  1.492921  1.623119

        404       405       406       407       408       409       410  \
0  0.000000  2.021082  0.256451  0.025724  2.811704  0.000000  0.552787
1  0.264902  0.831599  0.000000  2.590551  0.257478  0.956989  0.000000
2  1.285994  1.363408  0.623285  0.271699  0.000000  6.107898  0.382187
3  0.000000  3.580813  0.000000  2.099309  4.158195  0.167299  0.000000
4  0.132260  2.719237  0.391142  0.530853  4.428109  0.000000  3.414525

        411        412       413        414       415        416        417  \
0  0.000000   7.676309  1.219102  15.352860  0.000000   8.540647  19.741459
1  0.219144   7.311143  0.441619   4.332124  0.000000  14.580042  10.946703
2  0.000000   1.909684  0.711319   5.217044  0.335322   3.091615  13.099594
3  0.000000   0.450481  8.759375  13.571130  0.000000  15.925940  12.965261
4  0.000000  10.168381  0.832462   4.907068  3.031498   2.369122  42.428185

        418       419        420       421       422       423       424  \
0  0.000000  1.040017   1.177085  0.182446  0.090357  0.000000  4.849444
1  0.292312  1.335208   1.174249  0.142269  0.635535  0.000000  4.148453
2  0.000000  5.683343   4.258512  0.000000  0.883469  0.000000  4.553812
3  0.267207  0.117664   7.359328  0.000000  0.509095  0.801142  0.388840
4  1.059145  0.895257  13.338620  0.000000  4.416804  1.324670  2.685071

        425       426        427        428       429       430       431  \
0  0.264059  0.369499   0.276612  13.591271  0.091455  1.681300  0.749983
1  1.500191  0.000000   0.000000   5.092662  0.000000  1.407575  1.751954
2  1.184718  0.000000  11.704138   3.755493  5.037062  2.558465  0.000000
3  0.422575  0.000000   0.603760   7.127338  1.197330  0.731456  0.000000
4  0.000000  0.091829   0.043938  10.583324  0.890863  0.874705  0.684230

        432       433       434        435       436  437       438       439  \
0  1.616317  0.000000  1.205509   3.775423  1.493196  0.0  4.777265  0.227000
1  2.600412  0.560707  0.000000  13.162478  0.770256  0.0  8.141720  3.596904
2  1.330629  0.000000  0.298625   7.575385  0.075434  0.0  5.657953  4.211637
3  1.020255  0.000000  0.000000   3.931814  1.286294  0.0  5.253539  0.000000
4  4.555799  0.711946  0.554111   1.934971  0.000000  0.0  4.811326  0.136730

         440       441       442        443       444        445       446  \
0  20.661343  1.264974  1.817865   3.095562  2.068296  14.333957  0.000000
1  35.338596  1.155501  1.176568  25.710407  1.216553  11.421284  0.453162
2  33.635044  1.078662  5.162974  16.966583  0.338256  15.675865  0.000000
3  42.885727  0.297526  5.773962  10.496175  3.453733   8.161798  0.077753
4  35.674995  1.222096  0.118537  31.936209  4.427068   7.144698  0.000000

        447       448        449       450        451       452        453  \
0  0.000000  0.000000   6.156852  2.046904   5.240739  0.016992   5.305420
1  0.995387  5.517993   6.063191  0.865825   3.360670  0.987851   0.000000
2  0.000000  0.144292   0.231412  4.625395  10.266514  0.069346   0.000000
3  0.000000  0.329419   0.000000  0.038196   2.916266  0.773579   0.746041
4  0.000000  0.000000  14.067049  1.946674   0.236798  0.000000  10.181981

        454        455       456       457       458       459       460  \
0  0.000000   3.449175  0.448894  5.541336  0.096798  1.408330  1.026499
1  0.693182   9.450792  8.215945  4.990746  0.787756  0.531366  0.320673
2  0.264766   7.120823  4.088081  3.281134  0.618825  0.000000  0.737702
3  0.416004   2.458200  1.781745  2.340301  2.565139  0.125005  0.057738
4  6.294153  16.348385  0.729459  3.200897  3.829632  2.727303  4.207766

        461       462       463       464       465       466       467  \
0  0.023037  0.656764  1.337168  1.226299  2.316170  0.000000  0.000000
1  2.425077  0.027578  5.200702  1.016350  0.997208  0.729591  0.545410
2  3.129493  3.478486  0.864124  0.000000  9.749081  1.317710  0.929732
3  1.218027  0.000000  5.198735  0.047374  0.435599  1.253339  0.027170
4  8.019760  0.767232  1.068105  4.396734  6.323690  1.563376  1.028553

        468       469       470       471        472       473        474  \
0  0.375151  1.795415  0.707303  0.115279  19.998333  0.128960   0.495394
1  0.000556  1.593182  0.286321  0.000000   0.690598  1.219139   0.000000
2  0.000000  0.189996  1.228745  1.707588   2.381045  0.000000   3.721621
3  0.763804  1.457016  0.000000  0.615326   7.430285  0.000000   2.147947
4  0.535348  6.224815  0.889897  6.757069   4.935402  0.858948  12.067440

        475       476       477       478       479        480       481  \
0  0.000000  0.000477  2.770899  0.171643  0.440492   7.258298  4.097986
1  0.000000  0.000000  1.886817  0.000000  1.315044  12.145951  2.901029
2  0.456861  0.860002  0.524735  2.854396  0.110166   3.508886  0.873191
3  0.214846  0.292801  0.000000  2.015597  0.256574   4.512823  1.401941
4  0.458755  2.245364  0.000000  1.374461  7.610446   0.128499  5.565200

         482        483       484       485        486        487       488  \
0   8.936242   9.762069  0.077156  0.291878  13.252213   7.619406  1.093933
1   6.697383  18.075380  0.000000  0.000000   6.725642   9.175651  0.000000
2  10.438853  16.008392  1.829945  0.000000   6.353556   9.357926  0.000000
3  12.053901  19.815977  0.000000  0.000000   6.906956  44.622524  0.203918
4  15.103903  12.470869  0.000000  0.951658   9.790366  39.392525  1.871388

         489  490       491        492        493        494       495  \
0   0.203706  0.0  0.762282   1.687814  11.536532   6.425282  3.086462
1  10.629138  0.0  0.434412   0.892054   3.903777  15.775932  1.368484
2   0.663359  0.0  1.298626  15.336559   1.564542  11.154923  0.441460
3   1.910197  0.0  1.581871   9.242300   0.591414  11.744364  3.676615
4   6.188094  0.0  0.399971   1.941090  34.788895   2.017806  0.075354

        496       497       498       499       500       501       502  \
0  0.447673  1.105556  4.226742  0.664125  5.843996  1.081105  0.000000
1  0.565064  0.000000  1.369118  0.381792  0.000000  0.000000  0.220447
2  1.131390  0.243631  0.511774  5.467719  0.000000  0.000000  0.092866
3  0.139560  0.303121  2.621725  0.060500  0.000000  0.000000  0.000000
4  1.138237  4.379734  3.257020  1.826830  0.133392  0.000000  0.126074

        503       504       505       506       507        508       509  \
0  8.905491  0.000000  0.525374  1.273979  1.104460   6.974005  2.057517
1  7.528144  0.458274  0.173937  0.000000  3.225497  10.392344  0.000000
2  0.856133  1.051734  0.913523  0.000000  2.450588  13.462170  0.507899
3  5.697483  0.571326  0.000000  0.000000  0.132986   2.716901  0.000000
4  7.640112  0.850833  2.668359  4.523455  8.612874   6.427564  0.553557

        510       511
0  3.345634  1.990966
1  4.380680  1.921043
2  1.897847  0.420329
3  1.723300  0.000000
4  1.122496  0.268360

Eliminamos columnas del dataframe de listings que ya no nos hacen falta: listing_id y filename

Verificamos las columnas
Index(['price', 'vader_sentiment', 'host_response_rate', 'latitude',
       'longitude', 'bathrooms', 'bedrooms', 'beds', 'guests_included',
       'extra_people', 'minimum_nights', 'maximum_nights', 'availability_365',
       'number_of_reviews', 'host_total_listings_count', 'property_type',
       'room_type', 'bed_type', 'neighbourhood_cleansed',
       'cancellation_policy'],
      dtype='object')

Nuevo tamaño (8702, 20)

Unimos el dataset de atributos de los listing con el de las características de imágenes

Tamaño final del dataframe consolidado: (8702, 532)
Separamos train y test 80-20

Tamaño de train (6961, 532)
Tamaño de test (1741, 532)

-------------
Clasificación
-------------


2020-08-07 00:26:59.343712: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-07 00:26:59.356623: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe9e5854800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-07 00:26:59.356648: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version

Modelo simple de red neuronal de tres capas
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense (Dense)                (None, 64)                34048
_________________________________________________________________
dense_1 (Dense)              (None, 32)                2080
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 99
=================================================================
Total params: 36,227
Trainable params: 36,227
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
783/783 [==============================] - 1s 871us/step - loss: 0.6894 - accuracy: 0.7328 - val_loss: 0.6359 - val_accuracy: 0.7776
Epoch 2/50
783/783 [==============================] - 1s 725us/step - loss: 0.5954 - accuracy: 0.7731 - val_loss: 0.6085 - val_accuracy: 0.7489
Epoch 3/50
783/783 [==============================] - 1s 705us/step - loss: 0.5612 - accuracy: 0.7822 - val_loss: 0.5703 - val_accuracy: 0.7977
Epoch 4/50
783/783 [==============================] - 1s 714us/step - loss: 0.5356 - accuracy: 0.7934 - val_loss: 0.5785 - val_accuracy: 0.7661
Epoch 5/50
783/783 [==============================] - 1s 714us/step - loss: 0.5186 - accuracy: 0.7968 - val_loss: 0.5599 - val_accuracy: 0.7905
Epoch 6/50
783/783 [==============================] - 1s 720us/step - loss: 0.4995 - accuracy: 0.8060 - val_loss: 0.5743 - val_accuracy: 0.7920
Epoch 7/50
783/783 [==============================] - 1s 715us/step - loss: 0.4767 - accuracy: 0.8121 - val_loss: 0.5742 - val_accuracy: 0.8020
Epoch 8/50
783/783 [==============================] - 1s 736us/step - loss: 0.4621 - accuracy: 0.8129 - val_loss: 0.6030 - val_accuracy: 0.7791
Epoch 9/50
783/783 [==============================] - 1s 761us/step - loss: 0.4345 - accuracy: 0.8285 - val_loss: 0.5854 - val_accuracy: 0.7948
Epoch 10/50
783/783 [==============================] - 1s 789us/step - loss: 0.4057 - accuracy: 0.8383 - val_loss: 0.6195 - val_accuracy: 0.7791
Epoch 11/50
783/783 [==============================] - 1s 708us/step - loss: 0.3780 - accuracy: 0.8549 - val_loss: 0.6699 - val_accuracy: 0.7733
Epoch 12/50
783/783 [==============================] - 1s 788us/step - loss: 0.3447 - accuracy: 0.8613 - val_loss: 0.6519 - val_accuracy: 0.7848
Epoch 13/50
783/783 [==============================] - 1s 725us/step - loss: 0.3137 - accuracy: 0.8780 - val_loss: 0.6985 - val_accuracy: 0.7518
Epoch 14/50
783/783 [==============================] - 1s 715us/step - loss: 0.2803 - accuracy: 0.8902 - val_loss: 0.7141 - val_accuracy: 0.7633
Epoch 15/50
783/783 [==============================] - 1s 754us/step - loss: 0.2468 - accuracy: 0.9055 - val_loss: 0.7391 - val_accuracy: 0.7575
Epoch 16/50
783/783 [==============================] - 1s 752us/step - loss: 0.2157 - accuracy: 0.9199 - val_loss: 0.8300 - val_accuracy: 0.7561
Epoch 17/50
783/783 [==============================] - 1s 778us/step - loss: 0.1865 - accuracy: 0.9323 - val_loss: 0.9161 - val_accuracy: 0.7461
Epoch 18/50
783/783 [==============================] - 1s 790us/step - loss: 0.1588 - accuracy: 0.9440 - val_loss: 0.8964 - val_accuracy: 0.7532
Epoch 19/50
783/783 [==============================] - 1s 799us/step - loss: 0.1386 - accuracy: 0.9492 - val_loss: 0.9554 - val_accuracy: 0.7245
Epoch 20/50
783/783 [==============================] - 1s 713us/step - loss: 0.1182 - accuracy: 0.9583 - val_loss: 1.0614 - val_accuracy: 0.7575
Epoch 21/50
783/783 [==============================] - 1s 715us/step - loss: 0.1047 - accuracy: 0.9654 - val_loss: 1.1303 - val_accuracy: 0.7274
Epoch 22/50
783/783 [==============================] - 1s 728us/step - loss: 0.0952 - accuracy: 0.9679 - val_loss: 1.2623 - val_accuracy: 0.7532
Epoch 23/50
783/783 [==============================] - 1s 697us/step - loss: 0.0686 - accuracy: 0.9778 - val_loss: 1.4060 - val_accuracy: 0.7217
Epoch 24/50
783/783 [==============================] - 1s 683us/step - loss: 0.0667 - accuracy: 0.9784 - val_loss: 1.5107 - val_accuracy: 0.7274
Epoch 25/50
783/783 [==============================] - 1s 738us/step - loss: 0.0537 - accuracy: 0.9837 - val_loss: 1.5506 - val_accuracy: 0.7604
Epoch 26/50
783/783 [==============================] - 1s 712us/step - loss: 0.0722 - accuracy: 0.9756 - val_loss: 1.6117 - val_accuracy: 0.7274
Epoch 27/50
783/783 [==============================] - 1s 753us/step - loss: 0.0437 - accuracy: 0.9874 - val_loss: 1.7114 - val_accuracy: 0.7131
Epoch 28/50
783/783 [==============================] - 1s 723us/step - loss: 0.0801 - accuracy: 0.9716 - val_loss: 1.6957 - val_accuracy: 0.6915
Epoch 29/50
783/783 [==============================] - 1s 711us/step - loss: 0.0389 - accuracy: 0.9883 - val_loss: 1.5576 - val_accuracy: 0.7575
Epoch 30/50
783/783 [==============================] - 1s 713us/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 1.6290 - val_accuracy: 0.7374
Epoch 31/50
783/783 [==============================] - 1s 696us/step - loss: 0.0164 - accuracy: 0.9978 - val_loss: 1.9340 - val_accuracy: 0.7245
Epoch 32/50
783/783 [==============================] - 1s 694us/step - loss: 0.0665 - accuracy: 0.9757 - val_loss: 1.6915 - val_accuracy: 0.7475
Epoch 33/50
783/783 [==============================] - 1s 685us/step - loss: 0.0465 - accuracy: 0.9844 - val_loss: 1.7787 - val_accuracy: 0.7346
Epoch 34/50
783/783 [==============================] - 1s 701us/step - loss: 0.0123 - accuracy: 0.9979 - val_loss: 1.9400 - val_accuracy: 0.7489
Epoch 35/50
783/783 [==============================] - 1s 704us/step - loss: 0.0341 - accuracy: 0.9879 - val_loss: 2.2710 - val_accuracy: 0.6700
Epoch 36/50
783/783 [==============================] - 1s 711us/step - loss: 0.0525 - accuracy: 0.9829 - val_loss: 2.0882 - val_accuracy: 0.6743
Epoch 37/50
783/783 [==============================] - 1s 714us/step - loss: 0.0427 - accuracy: 0.9839 - val_loss: 2.0202 - val_accuracy: 0.7030
Epoch 38/50
783/783 [==============================] - 1s 734us/step - loss: 0.0393 - accuracy: 0.9874 - val_loss: 2.0804 - val_accuracy: 0.7403
Epoch 39/50
783/783 [==============================] - 1s 727us/step - loss: 0.0547 - accuracy: 0.9826 - val_loss: 2.0836 - val_accuracy: 0.7403
Epoch 40/50
783/783 [==============================] - 1s 748us/step - loss: 0.0207 - accuracy: 0.9935 - val_loss: 1.9737 - val_accuracy: 0.7389
Epoch 41/50
783/783 [==============================] - 1s 728us/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 2.1275 - val_accuracy: 0.7418
Epoch 42/50
783/783 [==============================] - 1s 753us/step - loss: 0.0042 - accuracy: 0.9997 - val_loss: 2.2304 - val_accuracy: 0.7374
Epoch 43/50
783/783 [==============================] - 1s 729us/step - loss: 0.0543 - accuracy: 0.9863 - val_loss: 2.1435 - val_accuracy: 0.7217
Epoch 44/50
783/783 [==============================] - 1s 719us/step - loss: 0.0690 - accuracy: 0.9762 - val_loss: 2.0604 - val_accuracy: 0.7174
Epoch 45/50
783/783 [==============================] - 1s 729us/step - loss: 0.0157 - accuracy: 0.9958 - val_loss: 2.0936 - val_accuracy: 0.7432
Epoch 46/50
783/783 [==============================] - 1s 743us/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 2.1277 - val_accuracy: 0.7346
Epoch 47/50
783/783 [==============================] - 1s 723us/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 2.0579 - val_accuracy: 0.7202
Epoch 48/50
783/783 [==============================] - 1s 726us/step - loss: 0.0712 - accuracy: 0.9788 - val_loss: 2.1303 - val_accuracy: 0.7389
Epoch 49/50
783/783 [==============================] - 1s 688us/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 2.3214 - val_accuracy: 0.7532
Epoch 50/50
783/783 [==============================] - 1s 705us/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 2.3500 - val_accuracy: 0.7303
55/55 [==============================] - 0s 575us/step - loss: 3.7327 - accuracy: 0.7059

La pérdida y el accuracy que se obtuvieron con imágenes no estructuradas fueron; Loss=8.13186645873205, Acc=0.5718390941619873

La pérdida y el accuracy obtenido ahora son: Loss=3.7326526641845703, Acc=0.705916166305542

Como se puede observar, del mismo modo que en el caso del estudio de solo con imágenes, el accuracy mejora en más de un 10%
Se reafirma que las imágenes estructuradas (imagen mosaico) mejoran los resultados.
También se debe tener en cuenta que se ha añadido el dato del VADER Sentiment que puede influir algo


Creamos modelo de clasificación con tres capas densas con regularización L1 y dos Dropout
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_3 (Dense)              (None, 64)                34048
_________________________________________________________________
dropout (Dropout)            (None, 64)                0
_________________________________________________________________
dense_4 (Dense)              (None, 32)                2080
_________________________________________________________________
dropout_1 (Dropout)          (None, 32)                0
_________________________________________________________________
dense_5 (Dense)              (None, 3)                 99
=================================================================
Total params: 36,227
Trainable params: 36,227
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
783/783 [==============================] - 1s 881us/step - loss: 1.5981 - accuracy: 0.6973 - val_loss: 0.8141 - val_accuracy: 0.7805
Epoch 2/50
783/783 [==============================] - 1s 739us/step - loss: 0.8292 - accuracy: 0.7577 - val_loss: 0.7605 - val_accuracy: 0.7791
Epoch 3/50
783/783 [==============================] - 1s 768us/step - loss: 0.8040 - accuracy: 0.7599 - val_loss: 0.7445 - val_accuracy: 0.7819
Epoch 4/50
783/783 [==============================] - 1s 810us/step - loss: 0.7923 - accuracy: 0.7573 - val_loss: 0.7393 - val_accuracy: 0.7805
Epoch 5/50
783/783 [==============================] - 1s 796us/step - loss: 0.7885 - accuracy: 0.7585 - val_loss: 0.7436 - val_accuracy: 0.7805
Epoch 6/50
783/783 [==============================] - 1s 767us/step - loss: 0.7759 - accuracy: 0.7602 - val_loss: 0.7255 - val_accuracy: 0.7805
Epoch 7/50
783/783 [==============================] - 1s 795us/step - loss: 0.7776 - accuracy: 0.7607 - val_loss: 0.7210 - val_accuracy: 0.7819
Epoch 8/50
783/783 [==============================] - 1s 812us/step - loss: 0.7672 - accuracy: 0.7597 - val_loss: 0.7624 - val_accuracy: 0.7805
Epoch 9/50
783/783 [==============================] - 1s 778us/step - loss: 0.7756 - accuracy: 0.7599 - val_loss: 0.7163 - val_accuracy: 0.7834
Epoch 10/50
783/783 [==============================] - 1s 765us/step - loss: 0.7675 - accuracy: 0.7593 - val_loss: 0.7358 - val_accuracy: 0.7819
Epoch 11/50
783/783 [==============================] - 1s 767us/step - loss: 0.7693 - accuracy: 0.7596 - val_loss: 0.7114 - val_accuracy: 0.7834
Epoch 12/50
783/783 [==============================] - 1s 778us/step - loss: 0.7659 - accuracy: 0.7599 - val_loss: 0.7157 - val_accuracy: 0.7834
Epoch 13/50
783/783 [==============================] - 1s 765us/step - loss: 0.7649 - accuracy: 0.7637 - val_loss: 0.7163 - val_accuracy: 0.7834
Epoch 14/50
783/783 [==============================] - 1s 767us/step - loss: 0.7638 - accuracy: 0.7594 - val_loss: 0.7112 - val_accuracy: 0.7834
Epoch 15/50
783/783 [==============================] - 1s 766us/step - loss: 0.7636 - accuracy: 0.7615 - val_loss: 0.7128 - val_accuracy: 0.7834
Epoch 16/50
783/783 [==============================] - 1s 755us/step - loss: 0.7650 - accuracy: 0.7605 - val_loss: 0.7064 - val_accuracy: 0.7834
Epoch 17/50
783/783 [==============================] - 1s 737us/step - loss: 0.7641 - accuracy: 0.7618 - val_loss: 0.7186 - val_accuracy: 0.7805
Epoch 18/50
783/783 [==============================] - 1s 758us/step - loss: 0.7586 - accuracy: 0.7602 - val_loss: 0.7073 - val_accuracy: 0.7834
Epoch 19/50
783/783 [==============================] - 1s 751us/step - loss: 0.7601 - accuracy: 0.7589 - val_loss: 0.7054 - val_accuracy: 0.7834
Epoch 20/50
783/783 [==============================] - 1s 752us/step - loss: 0.7599 - accuracy: 0.7604 - val_loss: 0.7093 - val_accuracy: 0.7834
Epoch 21/50
783/783 [==============================] - 1s 748us/step - loss: 0.7557 - accuracy: 0.7596 - val_loss: 0.7088 - val_accuracy: 0.7834
Epoch 22/50
783/783 [==============================] - 1s 784us/step - loss: 0.7572 - accuracy: 0.7605 - val_loss: 0.7024 - val_accuracy: 0.7834
Epoch 23/50
783/783 [==============================] - 1s 760us/step - loss: 0.7566 - accuracy: 0.7604 - val_loss: 0.7269 - val_accuracy: 0.7805
Epoch 24/50
783/783 [==============================] - 1s 756us/step - loss: 0.7567 - accuracy: 0.7609 - val_loss: 0.7090 - val_accuracy: 0.7834
Epoch 25/50
783/783 [==============================] - 1s 773us/step - loss: 0.7542 - accuracy: 0.7615 - val_loss: 0.7063 - val_accuracy: 0.7834
Epoch 26/50
783/783 [==============================] - 1s 754us/step - loss: 0.7572 - accuracy: 0.7609 - val_loss: 0.7205 - val_accuracy: 0.7805
Epoch 27/50
783/783 [==============================] - 1s 737us/step - loss: 0.7553 - accuracy: 0.7615 - val_loss: 0.7035 - val_accuracy: 0.7834
Epoch 28/50
783/783 [==============================] - 1s 752us/step - loss: 0.7525 - accuracy: 0.7609 - val_loss: 0.6984 - val_accuracy: 0.7834
Epoch 29/50
783/783 [==============================] - 1s 728us/step - loss: 0.7596 - accuracy: 0.7604 - val_loss: 0.7117 - val_accuracy: 0.7834
Epoch 30/50
783/783 [==============================] - 1s 748us/step - loss: 0.7525 - accuracy: 0.7637 - val_loss: 0.7009 - val_accuracy: 0.7834
Epoch 31/50
783/783 [==============================] - 1s 742us/step - loss: 0.7583 - accuracy: 0.7583 - val_loss: 0.7040 - val_accuracy: 0.7805
Epoch 32/50
783/783 [==============================] - 1s 740us/step - loss: 0.7605 - accuracy: 0.7623 - val_loss: 0.7070 - val_accuracy: 0.7834
Epoch 33/50
783/783 [==============================] - 1s 738us/step - loss: 0.7516 - accuracy: 0.7626 - val_loss: 0.7120 - val_accuracy: 0.7805
Epoch 34/50
783/783 [==============================] - 1s 738us/step - loss: 0.7497 - accuracy: 0.7634 - val_loss: 0.6993 - val_accuracy: 0.7834
Epoch 35/50
783/783 [==============================] - 1s 754us/step - loss: 0.7474 - accuracy: 0.7629 - val_loss: 0.7005 - val_accuracy: 0.7834
Epoch 36/50
783/783 [==============================] - 1s 756us/step - loss: 0.7563 - accuracy: 0.7628 - val_loss: 0.7122 - val_accuracy: 0.7834
Epoch 37/50
783/783 [==============================] - 1s 769us/step - loss: 0.7552 - accuracy: 0.7625 - val_loss: 0.7046 - val_accuracy: 0.7791
Epoch 38/50
783/783 [==============================] - 1s 760us/step - loss: 0.7579 - accuracy: 0.7607 - val_loss: 0.6983 - val_accuracy: 0.7834
Epoch 39/50
783/783 [==============================] - 1s 754us/step - loss: 0.7526 - accuracy: 0.7612 - val_loss: 0.6982 - val_accuracy: 0.7834
Epoch 40/50
783/783 [==============================] - 1s 745us/step - loss: 0.7530 - accuracy: 0.7615 - val_loss: 0.7081 - val_accuracy: 0.7819
Epoch 41/50
783/783 [==============================] - 1s 738us/step - loss: 0.7579 - accuracy: 0.7605 - val_loss: 0.7087 - val_accuracy: 0.7834
Epoch 42/50
783/783 [==============================] - 1s 749us/step - loss: 0.7560 - accuracy: 0.7626 - val_loss: 0.7034 - val_accuracy: 0.7834
Epoch 43/50
783/783 [==============================] - 1s 746us/step - loss: 0.7551 - accuracy: 0.7639 - val_loss: 0.7069 - val_accuracy: 0.7834
Epoch 44/50
783/783 [==============================] - 1s 742us/step - loss: 0.7504 - accuracy: 0.7637 - val_loss: 0.7054 - val_accuracy: 0.7834
Epoch 45/50
783/783 [==============================] - 1s 747us/step - loss: 0.7524 - accuracy: 0.7628 - val_loss: 0.7077 - val_accuracy: 0.7805
Epoch 46/50
783/783 [==============================] - 1s 737us/step - loss: 0.7603 - accuracy: 0.7594 - val_loss: 0.7010 - val_accuracy: 0.7834
Epoch 47/50
783/783 [==============================] - 1s 745us/step - loss: 0.7500 - accuracy: 0.7623 - val_loss: 0.7026 - val_accuracy: 0.7834
Epoch 48/50
783/783 [==============================] - 1s 746us/step - loss: 0.7541 - accuracy: 0.7644 - val_loss: 0.7042 - val_accuracy: 0.7834
Epoch 49/50
783/783 [==============================] - 1s 755us/step - loss: 0.7532 - accuracy: 0.7625 - val_loss: 0.6982 - val_accuracy: 0.7834
Epoch 50/50
783/783 [==============================] - 1s 747us/step - loss: 0.7538 - accuracy: 0.7597 - val_loss: 0.6981 - val_accuracy: 0.7834
55/55 [==============================] - 0s 544us/step - loss: 0.7072 - accuracy: 0.7760

La pérdida y el accuracy que se obtuvieron con imágenes no estructuradas fueron; Loss=1.5043120713069522, Acc=0.5086206793785095

La pérdida y el accuracy obtenido ahora son: Loss=0.7071892023086548, Acc=0.7759907841682434

Como se puede observar mejora los datos obtenidos de la anterior red, más simple.


-------------
Regresión
-------------



Modelo simple de red neuronal de tres capas
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_6 (Dense)              (None, 64)                34048
_________________________________________________________________
dense_7 (Dense)              (None, 32)                2080
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 33
=================================================================
Total params: 36,161
Trainable params: 36,161
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
783/783 [==============================] - 1s 766us/step - loss: 0.0024 - val_loss: 0.0012
Epoch 2/50
783/783 [==============================] - 1s 681us/step - loss: 0.0020 - val_loss: 0.0018
Epoch 3/50
783/783 [==============================] - 1s 686us/step - loss: 0.0020 - val_loss: 0.0012
Epoch 4/50
783/783 [==============================] - 1s 693us/step - loss: 0.0019 - val_loss: 0.0012
Epoch 5/50
783/783 [==============================] - 1s 685us/step - loss: 0.0019 - val_loss: 0.0011
Epoch 6/50
783/783 [==============================] - 1s 685us/step - loss: 0.0018 - val_loss: 0.0025
Epoch 7/50
783/783 [==============================] - 1s 681us/step - loss: 0.0018 - val_loss: 0.0011
Epoch 8/50
783/783 [==============================] - 1s 684us/step - loss: 0.0018 - val_loss: 0.0012
Epoch 9/50
783/783 [==============================] - 1s 684us/step - loss: 0.0018 - val_loss: 0.0011
Epoch 10/50
783/783 [==============================] - 1s 683us/step - loss: 0.0018 - val_loss: 0.0011
Epoch 11/50
783/783 [==============================] - 1s 683us/step - loss: 0.0017 - val_loss: 0.0013
Epoch 12/50
783/783 [==============================] - 1s 683us/step - loss: 0.0017 - val_loss: 0.0013
Epoch 13/50
783/783 [==============================] - 1s 686us/step - loss: 0.0016 - val_loss: 0.0012
Epoch 14/50
783/783 [==============================] - 1s 686us/step - loss: 0.0016 - val_loss: 0.0011
Epoch 15/50
783/783 [==============================] - 1s 685us/step - loss: 0.0015 - val_loss: 0.0013
Epoch 16/50
783/783 [==============================] - 1s 698us/step - loss: 0.0014 - val_loss: 0.0011
Epoch 17/50
783/783 [==============================] - 1s 686us/step - loss: 0.0014 - val_loss: 0.0011
Epoch 18/50
783/783 [==============================] - 1s 685us/step - loss: 0.0013 - val_loss: 0.0012
Epoch 19/50
783/783 [==============================] - 1s 688us/step - loss: 0.0013 - val_loss: 0.0013
Epoch 20/50
783/783 [==============================] - 1s 685us/step - loss: 0.0013 - val_loss: 9.6409e-04
Epoch 21/50
783/783 [==============================] - 1s 686us/step - loss: 0.0012 - val_loss: 0.0013
Epoch 22/50
783/783 [==============================] - 1s 689us/step - loss: 0.0012 - val_loss: 9.4207e-04
Epoch 23/50
783/783 [==============================] - 1s 686us/step - loss: 0.0011 - val_loss: 9.9390e-04
Epoch 24/50
783/783 [==============================] - 1s 685us/step - loss: 0.0011 - val_loss: 0.0010
Epoch 25/50
783/783 [==============================] - 1s 686us/step - loss: 0.0011 - val_loss: 9.8981e-04
Epoch 26/50
783/783 [==============================] - 1s 694us/step - loss: 0.0011 - val_loss: 9.3495e-04
Epoch 27/50
783/783 [==============================] - 1s 686us/step - loss: 0.0011 - val_loss: 0.0011
Epoch 28/50
783/783 [==============================] - 1s 686us/step - loss: 0.0010 - val_loss: 9.8777e-04
Epoch 29/50
783/783 [==============================] - 1s 684us/step - loss: 0.0010 - val_loss: 8.2702e-04
Epoch 30/50
783/783 [==============================] - 1s 686us/step - loss: 9.3826e-04 - val_loss: 8.7575e-04
Epoch 31/50
783/783 [==============================] - 1s 683us/step - loss: 9.1353e-04 - val_loss: 0.0010
Epoch 32/50
783/783 [==============================] - 1s 684us/step - loss: 8.7699e-04 - val_loss: 0.0012
Epoch 33/50
783/783 [==============================] - 1s 693us/step - loss: 8.8568e-04 - val_loss: 9.6865e-04
Epoch 34/50
783/783 [==============================] - 1s 686us/step - loss: 8.5575e-04 - val_loss: 8.0684e-04
Epoch 35/50
783/783 [==============================] - 1s 686us/step - loss: 8.0017e-04 - val_loss: 7.4338e-04
Epoch 36/50
783/783 [==============================] - 1s 689us/step - loss: 8.3360e-04 - val_loss: 7.1852e-04
Epoch 37/50
783/783 [==============================] - 1s 685us/step - loss: 8.0557e-04 - val_loss: 0.0012
Epoch 38/50
783/783 [==============================] - 1s 685us/step - loss: 7.8401e-04 - val_loss: 0.0013
Epoch 39/50
783/783 [==============================] - 1s 688us/step - loss: 7.5439e-04 - val_loss: 6.8485e-04
Epoch 40/50
783/783 [==============================] - 1s 683us/step - loss: 7.2845e-04 - val_loss: 9.3270e-04
Epoch 41/50
783/783 [==============================] - 1s 690us/step - loss: 6.8127e-04 - val_loss: 9.5179e-04
Epoch 42/50
783/783 [==============================] - 1s 685us/step - loss: 6.2523e-04 - val_loss: 6.3339e-04
Epoch 43/50
783/783 [==============================] - 1s 686us/step - loss: 6.6638e-04 - val_loss: 6.2949e-04
Epoch 44/50
783/783 [==============================] - 1s 689us/step - loss: 6.3710e-04 - val_loss: 0.0011
Epoch 45/50
783/783 [==============================] - 1s 687us/step - loss: 5.6809e-04 - val_loss: 6.5793e-04
Epoch 46/50
783/783 [==============================] - 1s 682us/step - loss: 5.3786e-04 - val_loss: 7.7782e-04
Epoch 47/50
783/783 [==============================] - 1s 690us/step - loss: 5.6276e-04 - val_loss: 7.4311e-04
Epoch 48/50
783/783 [==============================] - 1s 685us/step - loss: 5.0890e-04 - val_loss: 5.9212e-04
Epoch 49/50
783/783 [==============================] - 1s 685us/step - loss: 4.9177e-04 - val_loss: 6.6533e-04
Epoch 50/50
783/783 [==============================] - 1s 685us/step - loss: 4.7869e-04 - val_loss: 7.4794e-04
55/55 [==============================] - 0s 497us/step - loss: 0.0104
La pérdida obtenida con la misma red sin imágenes estructuradas fue Loss=0.037447664230148454
La pérdida obtenida actual obtenida con los datos de test es: Loss=0.010369366966187954
Como se puede observar la pérdida con las imágenes estructuradas en mosaico es bastante menor,
por lo que queda confirmado una vez más, que las redes mejoran notablemente con información estructurada.


CONCLUSIONES
------------


La VGG16 utilizada para la extracción de características de las imágenes de Tumbnail de las viviendas
no son correctas o no proprocionan la suficiente información para poder realizar tareas de clasificación
o regresión sobre los precios. Sin embargo, si se crean modelos para la clasificación correcta de tipos de
estancias y esta información se aplica de forma estructurada, los sistemas mejoran notablemente. En una red
de clasificación se ha obtenido un accuracy de uno 77%, lo cual permitiría, en algunos entornos, utilizar el
modelo como sistema de predicción.

Llama la atención lo precisos que son los modelos de clasificación de estancias utilizando ResNet50 a pesar de que imagenet nunca
ha realizado un estudio sobre este tipo de imágenes. Se concluye que los pesos que proporciona imagenet se pueden
utilizar para casi cualquier tipo de clasificación de imágenes.

Se podrían hacer más pruebas clasificación de estancias utilizando otras soluciones diferentes a ResNet50, como
Xception o MobileNet. Es muy probable que con estas soluciones se consiga mejorar algo, a pesar de que ResNet50
proporciona valores muy buenos.

Process finished with exit code 0

